{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f26c9-a8c4-4a4d-8209-60e8c9f3f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import math\n",
    "import sklearn.datasets\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import copy\n",
    "\n",
    "\n",
    "from nn_building_blocks import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57758d72-7711-426b-986d-302ceabac6db",
   "metadata": {},
   "source": [
    "# Binary Classification with Tanh in a Single Hidden Layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5214a6-9fc0-4b00-9612-a3c6a1eb29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "m = 400 # number of examples\n",
    "N = int(m/2) # number of points per class\n",
    "D = 2 # dimensionality\n",
    "X = np.zeros((m,D)) # data matrix where each row is a single example\n",
    "Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
    "a = 4 # maximum ray of the flower\n",
    "\n",
    "for j in range(2):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
    "    r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
    "    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    Y[ix] = j\n",
    "    \n",
    "X = X.T\n",
    "Y = Y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86777cb1-d529-48c4-a9ce-67c73c807824",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classification_layers_dims = (X.shape[0], 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505105a9-e720-4821-81d7-216a90a43590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_binary_1L_model(X, Y, layers_dims, optimizer=\"gd\", learning_rate = 0.0007, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = False, decay=None, decay_rate=1):\n",
    "\n",
    "    np.random.seed(3)\n",
    "    \n",
    "    L = len(layers_dims)             \n",
    "    costs = []                       \n",
    "    t = 0                            \n",
    "    m = X.shape[1]                  \n",
    "    lr_rates = []\n",
    "    learning_rate0 = learning_rate\n",
    "    \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    if optimizer != \"momentum\" and optimizer != \"adam\":\n",
    "        pass \n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        al, caches = forward_propagation(X, parameters, \"tanh\")\n",
    "        cost_avg = compute_cost_log_loss(al, Y)\n",
    "        grads = backward_propagation(X, Y, caches, \"tanh\")\n",
    "\n",
    "    \n",
    "        if optimizer != \"momentum\" and optimizer != \"adam\":\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        elif optimizer == \"momentum\":\n",
    "            parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "        elif optimizer == \"adam\":\n",
    "            t = t + 1 # Adam counter\n",
    "            parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                           t, learning_rate, beta1, beta2,  epsilon)\n",
    "\n",
    "            \n",
    "        if decay:\n",
    "            learning_rate = decay(learning_rate0, i, decay_rate)\n",
    "       \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "            if decay:\n",
    "                print(\"learning rate after epoch %i: %f\"%(i, learning_rate))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"model Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    # Parameters for prediction and grads for gradient checking\n",
    "    return parameters, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43ba0c-c60c-4264-a079-425ef7a965ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict1(parameters, X):\n",
    "    A2, cache = forward_propagation(X, parameters, \"tanh\")\n",
    "    prediction = (A2 > 0.5).astype(int)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0742de3-5c0c-4392-a9ab-f7cdfcfeb0df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd902c8-abfd-495f-b6ff-cdfb17d13d71",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_extra_datasets():  \n",
    "    N = 200\n",
    "    noisy_circles = sklearn.datasets.make_circles(n_samples=N, factor=.5, noise=.3)\n",
    "    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)\n",
    "    blobs = sklearn.datasets.make_blobs(n_samples=N, random_state=5, n_features=2, centers=6)\n",
    "    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)\n",
    "    no_structure = np.random.rand(N, 2), np.random.rand(N, 2)\n",
    "    \n",
    "    return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9e263-b586-4fe1-98fc-317c04be66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_parameters, gradients = model(X, Y, binary_classification_layers_dims, hidden_activation=\"tanh\", optimizer = \"gd\", learning_rate = 1.2, num_epochs=10000, print_cost=True, check_gradient=True, seed=1)\n",
    "\n",
    "# Predict\n",
    "predictions = predict1(binary_parameters, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a7936-b5e1-4dae-8a45-92c42eb3e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict1(binary_parameters, X)\n",
    "print ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55153a68-f7fc-4ff9-819e-4c8c6f7d5fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: predict1(binary_parameters, x.T), X, Y)\n",
    "plt.title(\"Decision Boundary for hidden layer size \" + str(4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92a9b2-e624-4e68-9bd3-8087702bf590",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Datasets\n",
    "# '''\n",
    "# Rerun the cells above after selecting a datasets and running this cell\n",
    "# '''\n",
    "\n",
    "# noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n",
    "\n",
    "# datasets = {\"noisy_circles\": noisy_circles,\n",
    "#             \"noisy_moons\": noisy_moons,\n",
    "#             \"blobs\": blobs,\n",
    "#             \"gaussian_quantiles\": gaussian_quantiles}\n",
    "\n",
    "# ### START CODE HERE ### (choose your dataset)\n",
    "# dataset = \"noisy_circles\"\n",
    "# ### END CODE HERE ###\n",
    "\n",
    "# X, Y = datasets[dataset]\n",
    "# X, Y = X.T, Y.reshape(1, Y.shape[0])\n",
    "\n",
    "# # make blobs binary\n",
    "# if dataset == \"blobs\":\n",
    "#     Y = Y%2\n",
    "\n",
    "# # Visualize the data\n",
    "# plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19136c9c-e6b8-43b5-8d5f-577ae197abb3",
   "metadata": {},
   "source": [
    "# Binary Image Classification (Cats vs Not Cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb35769-238c-48ec-80bd-ddf12de36a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = h5py.File('../datasets/train_catvnoncat.h5')\n",
    "train_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
    "train_y = np.array(train_dataset[\"train_set_y\"][:])\n",
    "train_y = train_y.reshape(1, -1)\n",
    "\n",
    "test_dataset = h5py.File('../datasets/test_catvnoncat.h5')\n",
    "test_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n",
    "test_y = np.array(test_dataset[\"test_set_y\"][:])\n",
    "test_y = test_y.reshape(1, -1)\n",
    "\n",
    "classes = np.array(test_dataset[\"list_classes\"][:])\n",
    "\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1)\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1)\n",
    "\n",
    "train_x = train_x_flatten / 255\n",
    "test_x = test_x_flatten / 255\n",
    "\n",
    "xtr = train_x.T\n",
    "xtt = test_x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d60d8-6a11-409c-8b8d-6a51a542321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a picture\n",
    "index = 10\n",
    "plt.imshow(train_x_orig[index])\n",
    "plt.show()\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb3ee7-0880-4705-87ab-fdd9a839dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"train_x's shape: \" + str(xtr.shape))\n",
    "print (\"test_x's shape: \" + str(xtt.shape))\n",
    "print (\"train_y's shape: \" + str(train_y.shape))\n",
    "print (\"test_y's shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f0edc-a253-44aa-95f1-9e84ac641479",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classification_layers_dims = (xtr.shape[0], 20, 7, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba6c880-da76-4df4-9360-cb449a443a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_image_classification_model(X, Y, layers_dims, optimizer=\"gd\", learning_rate = 0.0007, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = False, decay=None, decay_rate=1):\n",
    "\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    L = len(layers_dims)             \n",
    "    costs = []                       \n",
    "    t = 0                            \n",
    "    m = X.shape[1]                  \n",
    "    lr_rates = []\n",
    "    learning_rate0 = learning_rate\n",
    "    \n",
    "    parameters = initialize_parameters_xavier(layers_dims)\n",
    "\n",
    "    if optimizer != \"momentum\" and optimizer != \"adam\":\n",
    "        pass \n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        al, caches = forward_propagation(X, parameters)\n",
    "        cost_avg = compute_cost_log_loss(al, Y)\n",
    "        grads = backward_propagation(X, Y, caches)\n",
    "\n",
    "    \n",
    "        if optimizer != \"momentum\" and optimizer != \"adam\":\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        elif optimizer == \"momentum\":\n",
    "            parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "        elif optimizer == \"adam\":\n",
    "            t = t + 1 # Adam counter\n",
    "            parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                           t, learning_rate, beta1, beta2,  epsilon)\n",
    "\n",
    "            \n",
    "        if decay:\n",
    "            learning_rate = decay(learning_rate0, i, decay_rate)\n",
    "       \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "            if decay:\n",
    "                print(\"learning rate after epoch %i: %f\"%(i, learning_rate))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "                \n",
    "    # # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"model Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    # Parameters for prediction and grads for gradient checking\n",
    "    return parameters, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f471fa-5362-43d1-920a-a6efdcf8506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classification_parameters, gradients = model(xtr, train_y, image_classification_layers_dims, learning_rate=0.0075, num_epochs = 2500, print_cost=True, init=\"xavier\", seed=1, check_gradient=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ceff2f-114a-41c2-9602-7b6c990ca2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = predict(xtr, train_y, image_classification_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca61bb-6cd1-4ab2-82df-a55a98380f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = predict(xtt, test_y, image_classification_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a83dd-1441-4c6a-a358-a304c1f3483f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# my_image = \"goat-2775034_960_720-3704390797.jpg\"\n",
    "# my_label_y = [1] \n",
    "\n",
    "\n",
    "# num_px = 64\n",
    "# fname = my_image\n",
    "# image = np.array(Image.open(fname).resize((num_px, num_px)))\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "# image = image / 255.\n",
    "# image = image.reshape((1, num_px * num_px * 3)).T\n",
    "\n",
    "# my_predicted_image = predict(image, my_label_y, image_classification_parameters, print_accuracy=False)\n",
    "\n",
    "\n",
    "# print (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ded42-b88d-4ed5-9cf4-807d00357b49",
   "metadata": {},
   "source": [
    "# Testing Different Parameter Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02977642-7f98-461f-ba65-8b90069f5d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "train_x1, train_y1 = sklearn.datasets.make_circles(n_samples=300, noise=0.05)\n",
    "np.random.seed(2)\n",
    "test_x1, test_y1 = sklearn.datasets.make_circles(n_samples=100, noise=0.05)\n",
    "\n",
    "train_x1 = train_x1.T\n",
    "test_x1 = test_x1.T\n",
    "\n",
    "train_y1 = train_y1.reshape(1, -1)\n",
    "test_y1 = test_y1.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9d543-3e87-4fba-b403-8dab7f2e226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_param_layers_dims = [train_x1.shape[0], 10, 5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a8102-4c4d-4bd1-b2fd-60c75d747633",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def customized_initialize_parameters_model(X, Y, layers_dims, optimizer=\"gd\", learning_rate = 0.0007, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = False, decay=None, decay_rate=1 ,init=\"random\"):\n",
    "\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    L = len(layers_dims)             \n",
    "    costs = []                       \n",
    "    t = 0                            \n",
    "    m = X.shape[1]                  \n",
    "    lr_rates = []\n",
    "    learning_rate0 = learning_rate\n",
    "    \n",
    "    \n",
    "    if init == \"zeros\":\n",
    "        parameters = {}\n",
    "        L = len(layers_dims)\n",
    "    \n",
    "        for l in range(1, L):\n",
    "            parameters[\"W\"+str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))\n",
    "            parameters[\"b\"+str(l)] = np.zeros((layers_dims[l],1))\n",
    "    \n",
    "    if init == \"random\":\n",
    "        np.random.seed(3)\n",
    "        parameters = {}\n",
    "        L = len(layers_dims)\n",
    "    \n",
    "        for l in range(1, L):\n",
    "            parameters[\"W\"+str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) *10\n",
    "            parameters[\"b\"+str(l)] = np.zeros((layers_dims[l],1))\n",
    "\n",
    "    if init == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims)\n",
    "\n",
    "    \n",
    "    if optimizer != \"momentum\" and optimizer != \"adam\":\n",
    "        pass \n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        al, caches = forward_propagation(X, parameters)\n",
    "        cost_avg = compute_cost_log_loss(al, Y)\n",
    "        grads = backward_propagation(X, Y, caches)\n",
    "\n",
    "    \n",
    "        if optimizer != \"momentum\" and optimizer != \"adam\":\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        elif optimizer == \"momentum\":\n",
    "            parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "        elif optimizer == \"adam\":\n",
    "            t = t + 1 # Adam counter\n",
    "            parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                           t, learning_rate, beta1, beta2,  epsilon)\n",
    "\n",
    "            \n",
    "        if decay:\n",
    "            learning_rate = decay(learning_rate0, i, decay_rate)\n",
    "       \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "            if decay:\n",
    "                print(\"learning rate after epoch %i: %f\"%(i, learning_rate))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "                \n",
    "    # # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"model Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    # Parameters for prediction and grads for gradient checking\n",
    "    return parameters, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652fdb75-6a9c-4a3a-b4e8-8817dba93b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xavier_parameters, gradientss = model(train_x1, train_y1, init_param_layers_dims, init = \"xavier\", learning_rate=0.01, num_epochs=15000, print_cost=True, check_gradient=True)\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_x1, train_y1, xavier_parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_x1, test_y1, xavier_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fafe2a-55d3-436d-9744-9a779ac97f1b",
   "metadata": {},
   "source": [
    "# Testing Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae46ef-7e11-4b5e-8530-0b858b6c341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat(\"../datasets/data.mat\")\n",
    "train_x2 = data[\"X\"].T\n",
    "train_y2 = data[\"y\"].T\n",
    "test_x2 = data[\"Xval\"].T\n",
    "test_y2 = data[\"yval\"].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ce413-a7db-47bc-9e60-fdaab4abfde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_model_layers_dim = [train_x2.shape[0], 20, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2a2ba-0890-4a64-bbae-05e195c19aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_reg_model(X, Y, layers_dims, \n",
    "                        optimizer=\"gd\", learning_rate = 0.0007, num_epochs = 5000,\n",
    "                        momentum_beta = 0.9, adam_beta1 = 0.9, adam_beta2 = 0.999,  epsilon = 1e-8,\n",
    "                        init=\"random\", hidden_activation=\"relu\", output_activation=\"sigmoid\",\n",
    "                        lambd=0, keep_probs=1,\n",
    "                        decay=None, decay_rate=1, \n",
    "                        print_cost = False, seed=None):\n",
    "\n",
    "    assert(lambd ==0 or keep_probs==1)\n",
    "\n",
    "    \n",
    "    L = len(layers_dims)             \n",
    "    costs = []                       \n",
    "    t = 0                            \n",
    "    m = X.shape[1]                  \n",
    "    lr_rates = []\n",
    "    learning_rate0 = learning_rate\n",
    "    \n",
    "    \n",
    "    \n",
    "    if init == \"random\":\n",
    "        parameters = initialize_parameters(layers_dims, seed)\n",
    "    elif init == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims, seed)\n",
    "    elif init == \"xavier\":\n",
    "        parameters = initialize_parameters_xavier(layers_dims, seed)\n",
    "\n",
    "    \n",
    "    \n",
    "    if optimizer != \"momentum\" and optimizer != \"adam\":\n",
    "        pass \n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        if keep_probs == 1:\n",
    "            al, caches = forward_propagation(X, parameters, hidden_activation, output_activation)\n",
    "        elif keep_probs < 1:\n",
    "            al, caches = forward_propagation_with_dropout(X, parameters, hidden_activation, output_activation, keep_probs)\n",
    "        \n",
    "        if lambd == 0:\n",
    "            cost_avg = compute_cost_log_loss(al, Y)\n",
    "        elif lambd > 0:\n",
    "            cost_avg = compute_cost_with_regularization(al, Y, parameters, lambd)\n",
    "\n",
    "        if lambd >= 0 and keep_probs == 1:\n",
    "            grads = backward_propagation(X, Y, caches, hidden_activation, lambd)\n",
    "        elif keep_probs < 1:\n",
    "            grads = backward_propagation_with_dropout(X, Y, caches, keep_probs, al, hidden_activation)\n",
    "        \n",
    "\n",
    "    \n",
    "        if optimizer != \"momentum\" and optimizer != \"adam\":\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        elif optimizer == \"momentum\":\n",
    "            parameters, v = update_parameters_with_momentum(parameters, grads, v, momentum_beta, learning_rate)\n",
    "        elif optimizer == \"adam\":\n",
    "            t = t + 1 # Adam counter\n",
    "            parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                           t, learning_rate, adam_beta1, adam_beta2,  epsilon)\n",
    "\n",
    "            \n",
    "        if decay:\n",
    "            learning_rate = decay(learning_rate0, i, decay_rate)\n",
    "       \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "            if decay:\n",
    "                print(\"learning rate after epoch %i: %f\"%(i, learning_rate))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "                \n",
    "    # # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"model Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    # Parameters for prediction and grads for gradient checking\n",
    "    return parameters, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f41281-1d01-46a9-87ef-4aa9f4c9aa37",
   "metadata": {},
   "source": [
    "#### Unregularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cfe1db-2a6e-4862-9178-d7cdc6bc79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unreg_parameters, gradients = model(train_x2, train_y2, regularization_model_layers_dim, learning_rate=0.3, num_epochs=30000, print_cost=True, init=\"xavier\", seed=3, check_gradient=True)\n",
    "print (\"On the training set:\")\n",
    "predictions_train = predict(train_x2, train_y2, unreg_parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_x2, test_y2, unreg_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176ca486-ff6a-4b34-9f06-d472c83b9765",
   "metadata": {},
   "source": [
    "#### Regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d6f27-8194-4b3a-af38-fc4604005bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_parameters, gradients = model(train_x2, train_y2, regularization_model_layers_dim, lambd=0.7, learning_rate=0.3, num_epochs=30000, print_cost=True, init=\"xavier\", seed=3, check_gradient=True)\n",
    "print (\"On the training set:\")\n",
    "predictions_train = predict(train_x2, train_y2, reg_parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_x2, test_y2, reg_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c79c0-bc26-4a8f-b13a-dcf350af95f5",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e914afc-67c8-4bd3-8f6e-e956773e64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_parameters, gradients = model(train_x2, train_y2, regularization_model_layers_dim, keep_probs=0.86, learning_rate=0.3, \n",
    "                                      num_epochs=30000, print_cost=True, init=\"xavier\", seed=3, check_gradient=True)\n",
    "print (\"On the training set:\")\n",
    "predictions_train = predict(train_x2, train_y2, dropout_parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_x2, test_y2, dropout_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde8cff-e833-49cd-bba4-c85b5b98294e",
   "metadata": {},
   "source": [
    "# Optimization Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b0561f-9fcc-47b3-98e1-2858342d8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "train_X3, train_Y3 = sklearn.datasets.make_moons(n_samples=300, noise=.2) #300 #0.2 \n",
    "# Visualize the data\n",
    "plt.scatter(train_X3[:, 0], train_X3[:, 1], c=train_Y3, s=40, cmap=plt.cm.Spectral);\n",
    "plt.show()\n",
    "train_X3 = train_X3.T\n",
    "train_Y3 = train_Y3.reshape((1, train_Y3.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9121b7-25c3-4c75-9d05-2fde828783c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizing_layers_dims = [train_X3.shape[0], 5, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8071f48-dda6-4977-a492-7776d0f11b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parameters, gradients = model(train_X3, train_Y3, optimizing_layers_dims, optimizer = \"momentum\", learning_rate = 0.1, num_epochs=5000, \n",
    "                              decay=schedule_lr_decay, print_cost=True, init=\"he\", seed=3, check_gradient=True)\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X3, train_Y3, optimized_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff23c6ec-74b2-4df9-a4ad-4deb2bb122a2",
   "metadata": {},
   "source": [
    "# Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b02b7f-9dff-4bb8-995f-165e875bf3d0",
   "metadata": {},
   "source": [
    "#### Gradient Checking: Binary Classification with Tanh in a Single Hidden Layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260953ca-2ccb-4ffb-b4e9-e8f6c7cb2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_al, final_caches = forward_propagation(X, binary_parameters, hidden_activation=\"tanh\", output_activation=\"sigmoid\")\n",
    "\n",
    "final_grads = backward_propagation(X, Y, final_caches, hidden_activation=\"tanh\", lambd=0)\n",
    "\n",
    "difference = gradient_checking(binary_parameters, final_grads, X, Y, print_msg=True, hidden_activation=\"tanh\", output_activation=\"sigmoid\")\n",
    "\n",
    "print(f\"Gradient check difference: {difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6b76a-b0d4-4c4a-8b57-e405c1275878",
   "metadata": {},
   "source": [
    "#### Gradient Checking: image classification Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171df976-28d8-48d4-87b5-a205ee1fdbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_al, final_caches = forward_propagation(xtr[:,:1], image_classification_parameters, hidden_activation=\"relu\", output_activation=\"sigmoid\")\n",
    "\n",
    "final_grads = backward_propagation(xtr[:,:1], train_y[:,:1], final_caches, hidden_activation=\"relu\", lambd=0)\n",
    "\n",
    "difference = gradient_checking(image_classification_parameters, final_grads, xtr[:,:1], train_y[:,:1], print_msg=True, hidden_activation=\"relu\", output_activation=\"sigmoid\")\n",
    "\n",
    "print(f\"Gradient check difference: {difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072c01c-e329-42c0-a80a-4f4f534f398f",
   "metadata": {},
   "source": [
    "#### Gradient Checking: Different Parameter Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a19e05-a33e-4bbd-99ca-ff042fdfb886",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_al, final_caches = forward_propagation(train_x1, xavier_parameters, hidden_activation=\"relu\", output_activation=\"sigmoid\")\n",
    "\n",
    "final_grads = backward_propagation(train_x1, train_y1, final_caches, hidden_activation=\"relu\", lambd=0)\n",
    "\n",
    "difference = gradient_checking(xavier_parameters, final_grads, train_x1, train_y1, print_msg=True, hidden_activation=\"relu\", output_activation=\"sigmoid\")\n",
    "\n",
    "print(f\"Gradient check difference: {difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e34eaa-f154-4bf0-b3c3-1196a7fa36a6",
   "metadata": {},
   "source": [
    "#### Gradient Checking: Unregularized Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ce364-e32f-4aaf-942a-c630b0347708",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_al, final_caches = forward_propagation(train_x2, unreg_parameters, hidden_activation=\"relu\", output_activation=\"sigmoid\")\n",
    "\n",
    "final_grads = backward_propagation(train_x2, train_y2, final_caches, hidden_activation=\"relu\", lambd=0)\n",
    "\n",
    "difference = gradient_checking(unreg_parameters, final_grads, train_x2, train_y2, print_msg=True, hidden_activation=\"relu\", output_activation=\"sigmoid\")\n",
    "\n",
    "print(f\"Gradient check difference: {difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b09f37c-0de2-468a-893b-ac908eba14c4",
   "metadata": {},
   "source": [
    "#### Gradient Checking: Regularized Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e73651-e414-43fe-b0a2-260d72459c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_al, final_caches = forward_propagation(train_x2, reg_parameters, hidden_activation=\"relu\", output_activation=\"sigmoid\")\n",
    "\n",
    "final_grads = backward_propagation(train_x2, train_y2, final_caches, hidden_activation=\"relu\", lambd=0)\n",
    "\n",
    "difference = gradient_checking(reg_parameters, final_grads, train_x2, train_y2, print_msg=True, hidden_activation=\"relu\", output_activation=\"sigmoid\")\n",
    "\n",
    "print(f\"Gradient check difference: {difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f91a85-1f81-44c6-bfaf-fcb5bbbd1680",
   "metadata": {},
   "source": [
    "#### Gradient Checking: Dropout Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165ffd6-8c78-446f-90fb-fb9f614e0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_al, final_caches = forward_propagation(train_x2, dropout_parameters, hidden_activation=\"relu\", output_activation=\"sigmoid\")\n",
    "\n",
    "final_grads = backward_propagation(train_x2, train_y2, final_caches, hidden_activation=\"relu\", lambd=0)\n",
    "\n",
    "difference = gradient_checking(dropout_parameters, final_grads, train_x2, train_y2, print_msg=True, hidden_activation=\"relu\", output_activation=\"sigmoid\")\n",
    "\n",
    "print(f\"Gradient check difference: {difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5d7a7-fb78-49a2-abe7-4c83a29a1437",
   "metadata": {},
   "source": [
    "#### Gradient Checking: Different Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a7f57-ce89-4f46-8606-c7269d900c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_al, final_caches = forward_propagation(train_X3, optimized_parameters, hidden_activation=\"relu\", output_activation=\"sigmoid\")\n",
    "\n",
    "final_grads = backward_propagation(train_X3, train_Y3, final_caches, hidden_activation=\"relu\", lambd=0)\n",
    "\n",
    "difference = gradient_checking(optimized_parameters, final_grads, train_X3, train_Y3, print_msg=True, hidden_activation=\"relu\", output_activation=\"sigmoid\")\n",
    "\n",
    "print(f\"Gradient check difference: {difference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469610ec-6016-4146-8c8e-0e47c7a68561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (ML)",
   "language": "python",
   "name": "ml3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
